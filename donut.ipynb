{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/thu-ml/zhusuan.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/haowen-xu/tfsnippet.git@v0.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/NetManAIOps/donut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.18.5\n",
    "!pip install tensorflow==1.15.4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from donut import complete_timestamp, standardize_kpi\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Read the raw data.\n",
    "df = pd.read_csv(\"realKnownCause/ambient_temperature_system_failure.csv\")\n",
    "timestamp2 = df['timestamp'].to_numpy()\n",
    "values = df['value'].to_numpy()# If there is no label, simply use all zeros.\n",
    "labels = np.zeros_like(values, dtype=np.int32)\n",
    "\n",
    "timestamp = list()\n",
    "for s in timestamp2:\n",
    "    timestamp.append(int(time.mktime(time.strptime(s, '%Y-%m-%d %H:%M:%S'))))\n",
    "\n",
    "# Complete the timestamp, and obtain the missing point indicators.\n",
    "timestamp, missing, (values, labels) = \\\n",
    "    complete_timestamp(timestamp, (values, labels))\n",
    "\n",
    "# Split the training and testing data.\n",
    "test_portion = 0.3\n",
    "test_n = int(len(values) * test_portion)\n",
    "train_values, test_values = values[:-test_n], values[-test_n:]\n",
    "train_labels, test_labels = labels[:-test_n], labels[-test_n:]\n",
    "train_missing, test_missing = missing[:-test_n], missing[-test_n:]\n",
    "\n",
    "# Standardize the training and testing data.\n",
    "train_values, mean, std = standardize_kpi(\n",
    "    train_values, excludes=np.logical_or(train_labels, train_missing))\n",
    "test_values, _, _ = standardize_kpi(test_values, mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\tfsnippet\\utils\\scope.py:26: The name tf.VariableScope is deprecated. Please use tf.compat.v1.VariableScope instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from donut import Donut\n",
    "from tensorflow import keras as K\n",
    "from tfsnippet.modules import Sequential\n",
    "\n",
    "# We build the entire model within the scope of `model_vs`,\n",
    "# it should hold exactly all the variables of `model`, including\n",
    "# the variables created by Keras layers.\n",
    "with tf.variable_scope('model') as model_vs:\n",
    "    model = Donut(\n",
    "        h_for_p_x=Sequential([\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "        ]),\n",
    "        h_for_q_z=Sequential([\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "        ]),\n",
    "        x_dims=120,\n",
    "        z_dims=5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\donut\\training.py:110: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\donut\\training.py:116: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\tfsnippet\\utils\\reuse.py:56: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\donut\\model.py:26: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\zhusuan\\distributions\\univariate.py:167: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\donut\\training.py:128: The name tf.losses.get_regularization_loss is deprecated. Please use tf.compat.v1.losses.get_regularization_loss instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\tfsnippet\\utils\\session.py:106: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\donut\\training.py:154: The name tf.check_numerics is deprecated. Please use tf.debugging.check_numerics instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\donut\\training.py:168: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\donut\\training.py:170: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\donut\\training.py:174: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\tfsnippet\\utils\\session.py:71: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\tfsnippet\\utils\\session.py:222: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\tfsnippet\\utils\\session.py:153: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "Trainable Parameters                     (58,150 in total)\n",
      "----------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (120,)         120\n",
      "donut/p_x_given_z/x_mean/kernel         (100, 120)  12,000\n",
      "donut/p_x_given_z/x_std/bias            (120,)         120\n",
      "donut/p_x_given_z/x_std/kernel          (100, 120)  12,000\n",
      "donut/q_z_given_x/z_mean/bias           (5,)             5\n",
      "donut/q_z_given_x/z_mean/kernel         (100, 5)       500\n",
      "donut/q_z_given_x/z_std/bias            (5,)             5\n",
      "donut/q_z_given_x/z_std/kernel          (100, 5)       500\n",
      "sequential/forward/_0/dense/bias        (100,)         100\n",
      "sequential/forward/_0/dense/kernel      (5, 100)       500\n",
      "sequential/forward/_1/dense_1/bias      (100,)         100\n",
      "sequential/forward/_1/dense_1/kernel    (100, 100)  10,000\n",
      "sequential_1/forward/_0/dense_2/bias    (100,)         100\n",
      "sequential_1/forward/_0/dense_2/kernel  (120, 100)  12,000\n",
      "sequential_1/forward/_1/dense_3/bias    (100,)         100\n",
      "sequential_1/forward/_1/dense_3/kernel  (100, 100)  10,000\n",
      "\n",
      "[Epoch 8/256, Step 100, ETA 2m 35.81s] step time: 0.04138s (±0.2903s); valid time: 0.7224s; loss: 72945.9 (±721509); valid loss: 314.714 (*)\n",
      "[Epoch 10/256, Step 140, ETA 1m 57.83s] Learning rate decreased to 0.00075\n",
      "[Epoch 15/256, Step 200, ETA 1m 28.52s] step time: 0.00646s (±0.004665s); valid time: 0.01562s; loss: 97.8394 (±3.88102); valid loss: 351.604\n",
      "[Epoch 20/256, Step 280, ETA 1m 8.401s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 22/256, Step 300, ETA 1m 7.769s] step time: 0.008349s (±0.02533s); valid time: 0.2344s; loss: 90.7917 (±3.12968); valid loss: 253.099 (*)\n",
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "[Epoch 29/256, Step 400, ETA 57.97s] step time: 0.01031s (±0.04071s); valid time: 0.4086s; loss: 87.5013 (±3.02523); valid loss: 203.526 (*)\n",
      "[Epoch 30/256, Step 420, ETA 55.94s] Learning rate decreased to 0.00042187500000000005\n",
      "[Epoch 36/256, Step 500, ETA 51.07s] step time: 0.008768s (±0.0298s); valid time: 0.2969s; loss: 82.4824 (±3.2305); valid loss: 197.582 (*)\n",
      "[Epoch 40/256, Step 560, ETA 46.93s] Learning rate decreased to 0.00031640625000000006\n",
      "[Epoch 43/256, Step 600, ETA 45.91s] step time: 0.008252s (±0.02677s); valid time: 0.25s; loss: 79.1646 (±3.35985); valid loss: 183.455 (*)\n",
      "[Epoch 50/256, Step 700, ETA 42.04s] step time: 0.008436s (±0.02834s); valid time: 0.2812s; loss: 77.3746 (±3.23781); valid loss: 180.527 (*)\n",
      "[Epoch 50/256, Step 700, ETA 42.04s] Learning rate decreased to 0.00023730468750000005\n",
      "[Epoch 58/256, Step 800, ETA 37.96s] step time: 0.005916s (±0.007199s); valid time: 0s; loss: 75.1885 (±2.53847); valid loss: 183.4\n",
      "[Epoch 60/256, Step 840, ETA 36.58s] Learning rate decreased to 0.00017797851562500002\n",
      "[Epoch 65/256, Step 900, ETA 35.49s] step time: 0.008746s (±0.02829s); valid time: 0.2812s; loss: 74.3392 (±2.85661); valid loss: 156.307 (*)\n",
      "[Epoch 70/256, Step 980, ETA 33.11s] Learning rate decreased to 0.00013348388671875002\n",
      "[Epoch 72/256, Step 1000, ETA 32.65s] step time: 0.006419s (±0.007105s); valid time: 0.01562s; loss: 73.9909 (±2.81981); valid loss: 177.626\n",
      "[Epoch 79/256, Step 1100, ETA 30.82s] step time: 0.00919s (±0.03283s); valid time: 0.3281s; loss: 72.5881 (±3.21027); valid loss: 150.76 (*)\n",
      "[Epoch 80/256, Step 1120, ETA 30.37s] Learning rate decreased to 0.00010011291503906251\n",
      "[Epoch 86/256, Step 1200, ETA 28.56s] step time: 0.006229s (±0.007149s); valid time: 0s; loss: 71.9781 (±2.72189); valid loss: 159.976\n",
      "[Epoch 90/256, Step 1260, ETA 27.27s] Learning rate decreased to 7.508468627929689e-05\n",
      "[Epoch 93/256, Step 1300, ETA 26.52s] step time: 0.00576s (±0.006784s); valid time: 0.01562s; loss: 70.5705 (±2.47736); valid loss: 167.493\n",
      "[Epoch 100/256, Step 1400, ETA 24.62s] step time: 0.005648s (±0.007077s); valid time: 0.01562s; loss: 71.5176 (±2.7695); valid loss: 155.906\n",
      "[Epoch 100/256, Step 1400, ETA 24.62s] Learning rate decreased to 5.631351470947266e-05\n",
      "[Epoch 108/256, Step 1500, ETA 22.9s] step time: 0.005281s (±0.007118s); valid time: 0.01562s; loss: 70.9391 (±2.44098); valid loss: 151.588\n",
      "[Epoch 110/256, Step 1540, ETA 22.25s] Learning rate decreased to 4.22351360321045e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 115/256, Step 1600, ETA 21.66s] step time: 0.00908s (±0.02975s); valid time: 0.2969s; loss: 71.3056 (±2.52188); valid loss: 145.326 (*)\n",
      "[Epoch 120/256, Step 1680, ETA 20.41s] Learning rate decreased to 3.167635202407837e-05\n",
      "[Epoch 122/256, Step 1700, ETA 20.14s] step time: 0.00606s (±0.006877s); valid time: 0s; loss: 70.9033 (±2.87624); valid loss: 147.709\n",
      "[Epoch 129/256, Step 1800, ETA 18.75s] step time: 0.005772s (±0.00493s); valid time: 0s; loss: 70.1501 (±2.78843); valid loss: 153.837\n",
      "[Epoch 130/256, Step 1820, ETA 18.57s] Learning rate decreased to 2.3757264018058778e-05\n",
      "[Epoch 136/256, Step 1900, ETA 17.7s] step time: 0.008769s (±0.007003s); valid time: 0.01599s; loss: 69.3398 (±2.80185); valid loss: 149.946\n",
      "[Epoch 140/256, Step 1960, ETA 17.25s] Learning rate decreased to 1.7817948013544083e-05\n",
      "[Epoch 143/256, Step 2000, ETA 16.75s] step time: 0.01s (±0.005225s); valid time: 0.01563s; loss: 70.019 (±2.57234); valid loss: 148.816\n",
      "[Epoch 150/256, Step 2100, ETA 15.46s] step time: 0.005665s (±0.005832s); valid time: 0.01562s; loss: 70.2182 (±2.29678); valid loss: 146.324\n",
      "[Epoch 150/256, Step 2100, ETA 15.46s] Learning rate decreased to 1.3363461010158061e-05\n",
      "[Epoch 158/256, Step 2200, ETA 14.24s] step time: 0.006349s (±0.004756s); valid time: 0.01199s; loss: 69.1729 (±2.73697); valid loss: 146.923\n",
      "[Epoch 160/256, Step 2240, ETA 13.76s] Learning rate decreased to 1.0022595757618546e-05\n",
      "[Epoch 165/256, Step 2300, ETA 13.05s] step time: 0.006291s (±0.003924s); valid time: 0.003113s; loss: 69.8812 (±2.66281); valid loss: 145.674\n",
      "[Epoch 170/256, Step 2380, ETA 12.12s] Learning rate decreased to 7.51694681821391e-06\n",
      "[Epoch 172/256, Step 2400, ETA 12.04s] step time: 0.009317s (±0.02967s); valid time: 0.2823s; loss: 70.4558 (±2.71772); valid loss: 143.722 (*)\n",
      "[Epoch 179/256, Step 2500, ETA 11.06s] step time: 0.01007s (±0.03373s); valid time: 0.3357s; loss: 69.6517 (±2.78566); valid loss: 143.145 (*)\n",
      "[Epoch 180/256, Step 2520, ETA 10.84s] Learning rate decreased to 5.637710113660432e-06\n",
      "[Epoch 186/256, Step 2600, ETA 9.972s] step time: 0.00698s (±0.004889s); valid time: 0.01399s; loss: 69.2015 (±2.35358); valid loss: 144.168\n",
      "[Epoch 190/256, Step 2660, ETA 9.368s] Learning rate decreased to 4.228282585245324e-06\n",
      "[Epoch 193/256, Step 2700, ETA 9.012s] step time: 0.009918s (±0.003949s); valid time: 0.01799s; loss: 69.0562 (±2.54683); valid loss: 144.271\n",
      "[Epoch 200/256, Step 2800, ETA 7.903s] step time: 0.005976s (±0.001256s); valid time: 0.01099s; loss: 69.3102 (±2.83526); valid loss: 144.131\n",
      "[Epoch 200/256, Step 2800, ETA 7.903s] Learning rate decreased to 3.171211938933993e-06\n",
      "[Epoch 208/256, Step 2900, ETA 6.844s] step time: 0.006693s (±0.002671s); valid time: 0.01699s; loss: 68.4108 (±2.39374); valid loss: 144.104\n",
      "[Epoch 210/256, Step 2940, ETA 6.46s] Learning rate decreased to 2.3784089542004944e-06\n",
      "[Epoch 215/256, Step 3000, ETA 5.878s] step time: 0.01024s (±0.003987s); valid time: 0.02099s; loss: 69.4552 (±2.7236); valid loss: 143.332\n",
      "[Epoch 220/256, Step 3080, ETA 5.11s] Learning rate decreased to 1.7838067156503708e-06\n",
      "[Epoch 222/256, Step 3100, ETA 4.93s] step time: 0.01176s (±0.004389s); valid time: 0.02999s; loss: 69.7645 (±2.72802); valid loss: 143.846\n",
      "[Epoch 229/256, Step 3200, ETA 3.944s] step time: 0.01169s (±0.04745s); valid time: 0.4742s; loss: 69.8202 (±2.45584); valid loss: 143.105 (*)\n",
      "[Epoch 230/256, Step 3220, ETA 3.748s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 236/256, Step 3300, ETA 2.961s] step time: 0.01356s (±0.03255s); valid time: 0.3299s; loss: 69.6723 (±2.56186); valid loss: 142.881 (*)\n",
      "[Epoch 240/256, Step 3360, ETA 2.321s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 243/256, Step 3400, ETA 1.916s] step time: 0.008982s (±0.03129s); valid time: 0.3143s; loss: 69.2259 (±2.60137); valid loss: 142.228 (*)\n",
      "[Epoch 250/256, Step 3500, ETA 0.8733s] step time: 0.008357s (±0.003179s); valid time: 0.01199s; loss: 69.2363 (±2.5793); valid loss: 142.493\n",
      "[Epoch 250/256, Step 3500, ETA 0.8733s] Learning rate decreased to 7.525434581650002e-07\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Matteo\\AppData\\Local\\Temp\\tmpqdrf6mj_\\variables.dat-3400\n",
      "WARNING:tensorflow:From C:\\Users\\Matteo\\Anaconda3\\lib\\site-packages\\donut\\reconstruction.py:53: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from donut import DonutTrainer, DonutPredictor\n",
    "\n",
    "trainer = DonutTrainer(model=model, model_vs=model_vs)\n",
    "predictor = DonutPredictor(model)\n",
    "\n",
    "with tf.Session().as_default():\n",
    "    trainer.fit(train_values, train_labels, train_missing, mean, std)\n",
    "    test_score = predictor.get_score(test_values, test_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-591.5387 -0.3574451\n"
     ]
    }
   ],
   "source": [
    "print(min(test_score), max(test_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
